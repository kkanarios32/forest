% ["references"]
\title{PLANNING IN STOCHASTIC ENVIRONMENTS WITH A LEARNED MODEL}
\date{2022}
\author{Ioannis Antonoglou}\author{Julian Schrittwieser}\author{Sherjil Ozair}\author{Thomas Hubert}
\taxon{reference}
\meta{bibtex}{\startverb
@article{antonoglouPLANNINGSTOCHASTICENVIRONMENTS2022,
 title = {{{PLANNING IN STOCHASTIC ENVIRONMENTS WITH A LEARNED MODEL}}},
 author = {Antonoglou, Ioannis and Schrittwieser, Julian and Ozair, Sherjil and Hubert, Thomas},
 year = {2022},
 file = {/home/kellen/Zotero/storage/TGICFELW/Antonoglou et al. - 2022 - PLANNING IN STOCHASTIC ENVIRONMENTS WITH A LEARNED MODEL.pdf},
 langid = {english},
 abstract = {Model-based reinforcement learning has proven highly successful. However, learning a model in isolation from its use during planning is problematic in complex environments. To date, the most effective techniques have instead combined valueequivalent model learning with powerful tree-search methods. This approach is exemplified by MuZero, which has achieved state-of-the-art performance in a wide range of domains, from board games to visually rich environments, with discrete and continuous action spaces, in online and offline settings. However, previous instantiations of this approach were limited to the use of deterministic models. This limits their performance in environments that are inherently stochastic, partially observed, or so large and complex that they appear stochastic to a finite agent. In this paper we extend this approach to learn and plan with stochastic models. Specifically, we introduce a new algorithm, Stochastic MuZero, that learns a stochastic model incorporating afterstates, and uses this model to perform a stochastic tree search. Stochastic MuZero matched or exceeded the state of the art in a set of canonical single and multi-agent environments, including 2048 and backgammon, while maintaining the superhuman performance of standard MuZero in the game of Go.}
}
\stopverb}