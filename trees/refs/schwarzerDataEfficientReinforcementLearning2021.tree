% ["refs"]
\title{Data-Efficient Reinforcement Learning with Self-Predictive Representations}
\date{2021-05}
\author/literal{Max Schwarzer}\author/literal{Ankesh Anand}\author/literal{Rishab Goel}\author/literal{R. Devon Hjelm}\author/literal{Aaron Courville}\author/literal{Philip Bachman}
\taxon{Reference}
\meta{doi}{10.48550/arXiv.2007.05929}
\meta{external}{https://arxiv.org/abs/2007.05929}

\meta{bibtex}{\startverb
@misc{schwarzerDataEfficientReinforcementLearning2021,
 title = {Data-{{Efficient Reinforcement Learning}} with {{Self-Predictive Representations}}},
 author = {Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R. Devon and Courville, Aaron and Bachman, Philip},
 year = {2021},
 doi = {10.48550/arXiv.2007.05929},
 urldate = {2025-07-29},
 number = {arXiv:2007.05929},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/EGGHMFSL/Schwarzer et al. - 2021 - Data-Efficient Reinforcement Learning with Self-Pr.pdf},
 keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Self-Predictive Representations (SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55\% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. We've made the code associated with this work available at https://github.com/mila-iqia/spr.},
 primaryclass = {cs},
 eprint = {2007.05929},
 month = {May}
}
\stopverb}