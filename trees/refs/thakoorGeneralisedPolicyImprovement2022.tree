% ["refs"]
\title{Generalised Policy Improvement with Geometric Policy Composition}
\date{2022-06}
\taxon{Reference}
\meta{doi}{10.48550/arXiv.2206.08736}
\meta{external}{https://arxiv.org/abs/2206.08736}

\meta{bibtex}{\startverb
@misc{thakoorGeneralisedPolicyImprovement2022,
 title = {Generalised {{Policy Improvement}} with {{Geometric Policy Composition}}},
 author = {Thakoor, Shantanu and Rowland, Mark and Borsa, Diana and Dabney, Will and Munos, R{\'e}mi and Barreto, Andr{\'e}},
 year = {2022},
 doi = {10.48550/arXiv.2206.08736},
 urldate = {2025-02-16},
 number = {arXiv:2206.08736},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/7R2UW6U8/Thakoor et al. - 2022 - Generalised Policy Improvement with Geometric Policy Composition.pdf},
 keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {We introduce a method for policy improvement that interpolates between the greedy approach of value-based reinforcement learning (RL) and the full planning approach typical of model-based RL. The new method builds on the concept of a geometric horizon model (GHM, also known as a {$\gamma$}-model), which models the discounted statevisitation distribution of a given policy. We show that we can evaluate any non-Markov policy that switches between a set of base Markov policies with fixed probability by a careful composition of the base policy GHMs, without any additional learning. We can then apply generalised policy improvement (GPI) to collections of such nonMarkov policies to obtain a new Markov policy that will in general outperform its precursors. We provide a thorough theoretical analysis of this approach, develop applications to transfer and standard RL, and empirically demonstrate its effectiveness over standard GPI on a challenging deep RL continuous control task. We also provide an analysis of GHM training methods, proving a novel convergence result regarding previously proposed methods and showing how to train these models stably in deep RL settings.},
 primaryclass = {stat},
 eprint = {2206.08736},
 month = {June}
}
\stopverb}