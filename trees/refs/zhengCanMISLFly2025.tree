% ["refs"]
\title{Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning}
\date{2025-03}
\author/literal{Chongyi Zheng}\author/literal{Jens Tuyls}\author/literal{Joanne Peng}\author/literal{Benjamin Eysenbach}
\taxon{Reference}
\meta{doi}{10.48550/arXiv.2412.08021}
\meta{external}{https://arxiv.org/abs/2412.08021}

\meta{bibtex}{\startverb
@misc{zhengCanMISLFly2025,
 title = {Can a {{MISL Fly}}? {{Analysis}} and {{Ingredients}} for {{Mutual Information Skill Learning}}},
 author = {Zheng, Chongyi and Tuyls, Jens and Peng, Joanne and Eysenbach, Benjamin},
 year = {2025},
 doi = {10.48550/arXiv.2412.08021},
 urldate = {2025-07-30},
 number = {arXiv:2412.08021},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/PS6YC24C/Zheng et al. - 2025 - Can a MISL Fly Analysis and Ingredients for Mutua.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL). Our analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA.},
 primaryclass = {cs},
 eprint = {2412.08021},
 month = {March},
 shorttitle = {Can a {{MISL Fly}}?}
}
\stopverb}