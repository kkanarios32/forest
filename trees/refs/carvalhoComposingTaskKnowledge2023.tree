% ["refs"]
\title{Composing Task Knowledge with Modular Successor Feature Approximators}
\date{2023-08}
\author/literal{Wilka Carvalho}\author/literal{Angelos Filos}\author/literal{Richard L. Lewis}\author/literal{Honglak lee}\author/literal{Satinder Singh}
\taxon{Reference}
\meta{doi}{10.48550/arXiv.2301.12305}
\meta{external}{https://arxiv.org/abs/2301.12305}

\meta{bibtex}{\startverb
@misc{carvalhoComposingTaskKnowledge2023,
 title = {Composing {{Task Knowledge}} with {{Modular Successor Feature Approximators}}},
 author = {Carvalho, Wilka and Filos, Angelos and Lewis, Richard L. and {lee}, Honglak and Singh, Satinder},
 year = {2023},
 doi = {10.48550/arXiv.2301.12305},
 urldate = {2025-08-12},
 number = {arXiv:2301.12305},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/BGZJ6IYS/Carvalho et al. - 2023 - Composing Task Knowledge with Modular Successor Fe.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Recently, the Successor Features and Generalized Policy Improvement (SF\&GPI) framework has been proposed as a method for learning, composing, and transferring predictive knowledge and behavior. SF\&GPI works by having an agent learn predictive representations (SFs) that can be combined for transfer to new tasks with GPI. However, to be effective this approach requires state features that are useful to predict, and these state-features are typically hand-designed. In this work, we present a novel neural network architecture, ``Modular Successor Feature Approximators'' (MSFA), where modules both discover what is useful to predict, and learn their own predictive representations. We show that MSFA is able to better generalize compared to baseline architectures for learning SFs and modular architectures for learning state representations.},
 primaryclass = {cs},
 eprint = {2301.12305},
 month = {August}
}
\stopverb}