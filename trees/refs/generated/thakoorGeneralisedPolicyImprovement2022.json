[{"DOI": "10.48550/arXiv.2206.08736", "URL": "https://arxiv.org/abs/2206.08736", "abstract": "We introduce a method for policy improvement that interpolates between the greedy approach of value-based reinforcement learning (RL) and the full planning approach typical of model-based RL. The new method builds on the concept of a geometric horizon model (GHM, also known as a \\gamma-model), which models the discounted statevisitation distribution of a given policy. We show that we can evaluate any non-Markov policy that switches between a set of base Markov policies with fixed probability by a careful composition of the base policy GHMs, without any additional learning. We can then apply generalised policy improvement (GPI) to collections of such nonMarkov policies to obtain a new Markov policy that will in general outperform its precursors. We provide a thorough theoretical analysis of this approach, develop applications to transfer and standard RL, and empirically demonstrate its effectiveness over standard GPI on a challenging deep RL continuous control task. We also provide an analysis of GHM training methods, proving a novel convergence result regarding previously proposed methods and showing how to train these models stably in deep RL settings.", "accessed": {"date-parts": [[2025, 2, 16]]}, "author": [{"family": "Thakoor", "given": "Shantanu"}, {"family": "Rowland", "given": "Mark"}, {"family": "Borsa", "given": "Diana"}, {"family": "Dabney", "given": "Will"}, {"family": "Munos", "given": "R\u00e9mi"}, {"family": "Barreto", "given": "Andr\u00e9"}], "id": "thakoorGeneralisedPolicyImprovement2022", "issued": {"date-parts": [[2022, 6]]}, "keyword": "Computer Science - Machine Learning,Statistics - Machine Learning", "language": "en-US", "number": "arXiv:2206.08736", "publisher": "arXiv", "title": "Generalised Policy Improvement with Geometric Policy Composition", "type": "", "original_bibtex": "@misc{thakoorGeneralisedPolicyImprovement2022,\n title = {Generalised {{Policy Improvement}} with {{Geometric Policy Composition}}},\n author = {Thakoor, Shantanu and Rowland, Mark and Borsa, Diana and Dabney, Will and Munos, R{\\'e}mi and Barreto, Andr{\\'e}},\n year = {2022},\n doi = {10.48550/arXiv.2206.08736},\n urldate = {2025-02-16},\n number = {arXiv:2206.08736},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/7R2UW6U8/Thakoor et al. - 2022 - Generalised Policy Improvement with Geometric Policy Composition.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We introduce a method for policy improvement that interpolates between the greedy approach of value-based reinforcement learning (RL) and the full planning approach typical of model-based RL. The new method builds on the concept of a geometric horizon model (GHM, also known as a {$\\gamma$}-model), which models the discounted statevisitation distribution of a given policy. We show that we can evaluate any non-Markov policy that switches between a set of base Markov policies with fixed probability by a careful composition of the base policy GHMs, without any additional learning. We can then apply generalised policy improvement (GPI) to collections of such nonMarkov policies to obtain a new Markov policy that will in general outperform its precursors. We provide a thorough theoretical analysis of this approach, develop applications to transfer and standard RL, and empirically demonstrate its effectiveness over standard GPI on a challenging deep RL continuous control task. We also provide an analysis of GHM training methods, proving a novel convergence result regarding previously proposed methods and showing how to train these models stably in deep RL settings.},\n primaryclass = {stat},\n eprint = {2206.08736},\n month = {June}\n}\n"}]