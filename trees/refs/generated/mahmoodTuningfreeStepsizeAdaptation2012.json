[{"DOI": "10.1109/ICASSP.2012.6288330", "ISBN": "978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5", "abstract": "Incremental learning algorithms based on gradient descent are effective and popular in online supervised learning, reinforcement learning, signal processing, and many other application areas. An oft-noted drawback of these algorithms is that they include a step-size parameter that needs to be tuned for best performance, which may require manual intervention and significant domain knowledge or additional data. In many cases, an entire vector of step-size parameters (e.g., one for each input feature) needs to be tuned in order to attain the best performance of the algorithm. To address this, several methods have been proposed for adapting step sizes online. For example, Sutton\u2019s IDBD method can find the best vector step size for the LMS algorithm, and Schraudolph\u2019s ELK1 method, an extension of IDBD to neural networks, has proven effective on large applications, such as 3D hand tracking. However, to date all such step-size adaptation methods have included a tunable step-size parameter of their own, which we call the meta-step-size parameter. In this paper we show that the performance of existing step-size adaptation methods are strongly dependent on the choice of their meta-step-size parameter and that their meta-step-size parameter cannot be set reliably in a problem-independent way. We introduce a series of modifications and normalizations to the IDBD method that together eliminate the need to tune the meta-step-size parameter to the particular problem. We show that the resulting overall algorithm, called Autostep, performs as well or better than the existing step-size adaptation methods on a number of idealized and robot prediction problems and does not require any tuning of its meta-step-size parameter. The ideas behind Autostep are not restricted to the IDBD method and the same principles are potentially applicable to other incremental learning settings, such as reinforcement learning.", "accessed": {"date-parts": [[2025, 2, 5]]}, "author": [{"family": "Mahmood", "given": "Ashique Rupam"}, {"family": "Sutton", "given": "Richard S."}, {"family": "Degris", "given": "Thomas"}, {"family": "Pilarski", "given": "Patrick M."}], "container-title": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "id": "mahmoodTuningfreeStepsizeAdaptation2012", "issued": {"date-parts": [[2012, 3]]}, "language": "en-US", "page": "2121-2124", "publisher": "IEEE", "publisher-place": "Kyoto, Japan", "title": "Tuning-free step-size adaptation", "type": "paper-conference", "original_bibtex": "@inproceedings{mahmoodTuningfreeStepsizeAdaptation2012,\n title = {Tuning-Free Step-Size Adaptation},\n author = {Mahmood, Ashique Rupam and Sutton, Richard S. and Degris, Thomas and Pilarski, Patrick M.},\n year = {2012},\n isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},\n doi = {10.1109/ICASSP.2012.6288330},\n urldate = {2025-02-05},\n booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},\n pages = {2121--2124},\n publisher = {IEEE},\n address = {Kyoto, Japan},\n file = {/home/kellen/Zotero/storage/ER798P67/Mahmood et al. - 2012 - Tuning-free step-size adaptation.pdf},\n langid = {english},\n abstract = {Incremental learning algorithms based on gradient descent are effective and popular in online supervised learning, reinforcement learning, signal processing, and many other application areas. An oft-noted drawback of these algorithms is that they include a step-size parameter that needs to be tuned for best performance, which may require manual intervention and significant domain knowledge or additional data. In many cases, an entire vector of step-size parameters (e.g., one for each input feature) needs to be tuned in order to attain the best performance of the algorithm. To address this, several methods have been proposed for adapting step sizes online. For example, Sutton's IDBD method can find the best vector step size for the LMS algorithm, and Schraudolph's ELK1 method, an extension of IDBD to neural networks, has proven effective on large applications, such as 3D hand tracking. However, to date all such step-size adaptation methods have included a tunable step-size parameter of their own, which we call the meta-step-size parameter. In this paper we show that the performance of existing step-size adaptation methods are strongly dependent on the choice of their meta-step-size parameter and that their meta-step-size parameter cannot be set reliably in a problem-independent way. We introduce a series of modifications and normalizations to the IDBD method that together eliminate the need to tune the meta-step-size parameter to the particular problem. We show that the resulting overall algorithm, called Autostep, performs as well or better than the existing step-size adaptation methods on a number of idealized and robot prediction problems and does not require any tuning of its meta-step-size parameter. The ideas behind Autostep are not restricted to the IDBD method and the same principles are potentially applicable to other incremental learning settings, such as reinforcement learning.},\n month = {March}\n}\n"}]