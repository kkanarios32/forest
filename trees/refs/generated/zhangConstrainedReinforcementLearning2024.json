[{"DOI": "10.48550/arXiv.2403.14508", "URL": "https://arxiv.org/abs/2403.14508", "abstract": "Reinforcement Learning (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined. However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously. Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms. Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available. We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier function to an additional safety critic. It implements an adaptive penalty for policy learning and alleviates the numerical issues that are known to complicate the application of the log barrier function method. As a result, we show that with CSAC-LB, we achieve state-of-the-art performance on several constrained control tasks with different levels of difficulty and evaluate our methods in a locomotion task on a real quadruped robot platform.", "accessed": {"date-parts": [[2025, 2, 3]]}, "author": [{"family": "Zhang", "given": "Baohe"}, {"family": "Zhang", "given": "Yuan"}, {"family": "Frison", "given": "Lilli"}, {"family": "Brox", "given": "Thomas"}, {"family": "B\u00f6decker", "given": "Joschka"}], "id": "zhangConstrainedReinforcementLearning2024", "issued": {"date-parts": [[2024, 3]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control", "language": "en-US", "number": "arXiv:2403.14508", "publisher": "arXiv", "title": "Constrained Reinforcement Learning with Smoothed Log Barrier Function", "type": "", "original_bibtex": "@misc{zhangConstrainedReinforcementLearning2024,\n title = {Constrained {{Reinforcement Learning}} with {{Smoothed Log Barrier Function}}},\n author = {Zhang, Baohe and Zhang, Yuan and Frison, Lilli and Brox, Thomas and B{\\\"o}decker, Joschka},\n year = {2024},\n doi = {10.48550/arXiv.2403.14508},\n urldate = {2025-02-03},\n number = {arXiv:2403.14508},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/GLQ2NBB3/Zhang et al. - 2024 - Constrained Reinforcement Learning with Smoothed Log Barrier Function.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Reinforcement Learning (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined. However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously. Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms. Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available. We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier function to an additional safety critic. It implements an adaptive penalty for policy learning and alleviates the numerical issues that are known to complicate the application of the log barrier function method. As a result, we show that with CSAC-LB, we achieve state-of-the-art performance on several constrained control tasks with different levels of difficulty and evaluate our methods in a locomotion task on a real quadruped robot platform.},\n primaryclass = {cs},\n eprint = {2403.14508},\n month = {March}\n}\n"}]