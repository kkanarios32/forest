[{"abstract": "How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.", "accessed": {"date-parts": [[2025, 2, 16]]}, "author": [{"family": "Eysenbach", "given": "Benjamin"}, {"family": "Salakhutdinov", "given": "Ruslan"}, {"family": "Levine", "given": "Sergey"}], "container-title": "International Conference on Learning Representations", "id": "eysenbachInformationGeometryUnsupervised2021", "issued": {"date-parts": [[2021, 10]]}, "language": "en-US", "title": "The Information Geometry of Unsupervised Reinforcement Learning", "type": "paper-conference", "original_bibtex": "@inproceedings{eysenbachInformationGeometryUnsupervised2021,\n title = {The {{Information Geometry}} of {{Unsupervised Reinforcement Learning}}},\n author = {Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey},\n year = {2021},\n urldate = {2025-02-16},\n booktitle = {International {{Conference}} on {{Learning Representations}}},\n file = {/home/kellen/Zotero/storage/VE3YH9VL/Eysenbach et al. - 2021 - The Information Geometry of Unsupervised Reinforcement Learning.pdf},\n langid = {english},\n abstract = {How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.},\n month = {October}\n}\n"}]