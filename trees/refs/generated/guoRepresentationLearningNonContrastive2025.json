[{"DOI": "10.48550/arXiv.2504.16667", "URL": "https://arxiv.org/abs/2504.16667", "abstract": "Labeling data is often very time consuming and expensive, leaving us with a majority of unlabeled data. Self-supervised representation learning methods such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very successful at learning meaningful latent representations from unlabeled image data, resulting in much more general and transferable representations for downstream tasks. Broadly, self-supervised methods fall into two types: 1) Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as BYOL. Contrastive methods are generally trying to maximize mutual information between related data points, so they need to compare every data point to every other data point, resulting in high variance, and thus requiring large batch sizes to work well. Non-contrastive methods like BYOL have much lower variance as they do not need to make pairwise comparisons, but are much trickier to implement as they have the possibility of collapsing to a constant vector. In this paper, we aim to develop a self-supervised objective that combines the strength of both types. We start with a particular contrastive method called the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we convert it into a more general non-contrastive form; this removes the pairwise comparisons resulting in lower variance, but keeps the mutual information formulation of the contrastive method preventing collapse. We call our new objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by learning image representations on ImageNet (similar to SimCLR and BYOL) and show that it consistently improves upon the Spectral Contrastive loss baseline.", "accessed": {"date-parts": [[2025, 5, 29]]}, "author": [{"family": "Guo", "given": "Zhaohan Daniel"}, {"family": "Pires", "given": "Bernardo Avila"}, {"family": "Khetarpal", "given": "Khimya"}, {"family": "Schuurmans", "given": "Dale"}, {"family": "Dai", "given": "Bo"}], "id": "guoRepresentationLearningNonContrastive2025", "issued": {"date-parts": [[2025, 4]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning", "number": "arXiv:2504.16667", "publisher": "arXiv", "title": "Representation Learning via Non-Contrastive Mutual Information", "type": "", "original_bibtex": "@misc{guoRepresentationLearningNonContrastive2025,\n title = {Representation {{Learning}} via {{Non-Contrastive Mutual Information}}},\n author = {Guo, Zhaohan Daniel and Pires, Bernardo Avila and Khetarpal, Khimya and Schuurmans, Dale and Dai, Bo},\n year = {2025},\n doi = {10.48550/arXiv.2504.16667},\n urldate = {2025-05-29},\n number = {arXiv:2504.16667},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/FX33U8RF/Guo et al. - 2025 - Representation Learning via Non-Contrastive Mutual Information.pdf;/home/kellen/Zotero/storage/MYI2C56X/2504.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Labeling data is often very time consuming and expensive, leaving us with a majority of unlabeled data. Self-supervised representation learning methods such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very successful at learning meaningful latent representations from unlabeled image data, resulting in much more general and transferable representations for downstream tasks. Broadly, self-supervised methods fall into two types: 1) Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as BYOL. Contrastive methods are generally trying to maximize mutual information between related data points, so they need to compare every data point to every other data point, resulting in high variance, and thus requiring large batch sizes to work well. Non-contrastive methods like BYOL have much lower variance as they do not need to make pairwise comparisons, but are much trickier to implement as they have the possibility of collapsing to a constant vector. In this paper, we aim to develop a self-supervised objective that combines the strength of both types. We start with a particular contrastive method called the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we convert it into a more general non-contrastive form; this removes the pairwise comparisons resulting in lower variance, but keeps the mutual information formulation of the contrastive method preventing collapse. We call our new objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by learning image representations on ImageNet (similar to SimCLR and BYOL) and show that it consistently improves upon the Spectral Contrastive loss baseline.},\n primaryclass = {cs},\n eprint = {2504.16667},\n month = {April}\n}\n"}]