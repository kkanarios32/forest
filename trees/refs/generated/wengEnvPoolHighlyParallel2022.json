[{"DOI": "10.48550/arXiv.2206.10558", "URL": "https://arxiv.org/abs/2206.10558", "abstract": "There has been significant progress in developing reinforcement learning (RL) training systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others, aim to improve the system\u2019s overall throughput. In this paper, we aim to address a common bottleneck in the RL training system, i.e., parallel environment execution, which is often the slowest part of the whole system but receives little attention. With a curated design for paralleling RL environments, we have improved the RL environment simulation speed across different hardware setups, ranging from a laptop and a modest workstation, to a high-end machine such as NVIDIA DGX-A100. On a high-end machine, EnvPool achieves one million frames per second for the environment execution on Atari environments and three million frames per second on MuJoCo environments. When running EnvPool on a laptop, the speed is 2.8 that of the Python subprocess. Moreover, great compatibility with existing RL training libraries has been demonstrated in the open-sourced community, including CleanRL, rl_games, DeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas at a much faster pace and has great potential to become the de facto RL environment execution engine. Example runs show that it only takes five minutes to train agents to play Atari Pong and MuJoCo Ant on a laptop. EnvPool is open-sourced at https://github.com/sail-sg/envpool.", "accessed": {"date-parts": [[2025, 2, 17]]}, "author": [{"family": "Weng", "given": "Jiayi"}, {"family": "Lin", "given": "Min"}, {"family": "Huang", "given": "Shengyi"}, {"family": "Liu", "given": "Bo"}, {"family": "Makoviichuk", "given": "Denys"}, {"family": "Makoviychuk", "given": "Viktor"}, {"family": "Liu", "given": "Zichen"}, {"family": "Song", "given": "Yufan"}, {"family": "Luo", "given": "Ting"}, {"family": "Jiang", "given": "Yukun"}, {"family": "Xu", "given": "Zhongwen"}, {"family": "Yan", "given": "Shuicheng"}], "id": "wengEnvPoolHighlyParallel2022", "issued": {"date-parts": [[2022, 10]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance,Computer Science - Robotics", "language": "en-US", "number": "arXiv:2206.10558", "publisher": "arXiv", "title": "EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine", "title-short": "EnvPool", "type": "", "original_bibtex": "@misc{wengEnvPoolHighlyParallel2022,\n title = {{{EnvPool}}: {{A Highly Parallel Reinforcement Learning Environment Execution Engine}}},\n author = {Weng, Jiayi and Lin, Min and Huang, Shengyi and Liu, Bo and Makoviichuk, Denys and Makoviychuk, Viktor and Liu, Zichen and Song, Yufan and Luo, Ting and Jiang, Yukun and Xu, Zhongwen and Yan, Shuicheng},\n year = {2022},\n doi = {10.48550/arXiv.2206.10558},\n urldate = {2025-02-17},\n number = {arXiv:2206.10558},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/UEN5NUYA/Weng et al. - 2022 - EnvPool A Highly Parallel Reinforcement Learning Environment Execution Engine.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance,Computer Science - Robotics},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {There has been significant progress in developing reinforcement learning (RL) training systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others, aim to improve the system's overall throughput. In this paper, we aim to address a common bottleneck in the RL training system, i.e., parallel environment execution, which is often the slowest part of the whole system but receives little attention. With a curated design for paralleling RL environments, we have improved the RL environment simulation speed across different hardware setups, ranging from a laptop and a modest workstation, to a high-end machine such as NVIDIA DGX-A100. On a high-end machine, EnvPool achieves one million frames per second for the environment execution on Atari environments and three million frames per second on MuJoCo environments. When running EnvPool on a laptop, the speed is 2.8{\\texttimes} that of the Python subprocess. Moreover, great compatibility with existing RL training libraries has been demonstrated in the open-sourced community, including CleanRL, rl\\_games, DeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas at a much faster pace and has great potential to become the de facto RL environment execution engine. Example runs show that it only takes five minutes to train agents to play Atari Pong and MuJoCo Ant on a laptop. EnvPool is open-sourced at https://github.com/sail-sg/envpool.},\n primaryclass = {cs},\n eprint = {2206.10558},\n month = {October},\n shorttitle = {{{EnvPool}}}\n}\n"}]