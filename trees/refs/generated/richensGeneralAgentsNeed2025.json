[{"DOI": "10.48550/arXiv.2506.01622", "URL": "https://arxiv.org/abs/2506.01622", "abstract": "Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent\u2019s policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.", "accessed": {"date-parts": [[2025, 6, 5]]}, "author": [{"family": "Richens", "given": "Jonathan"}, {"family": "Abel", "given": "David"}, {"family": "Bellot", "given": "Alexis"}, {"family": "Everitt", "given": "Tom"}], "id": "richensGeneralAgentsNeed2025", "issued": {"date-parts": [[2025, 6]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning", "number": "arXiv:2506.01622", "publisher": "arXiv", "title": "General agents need world models", "type": "", "original_bibtex": "@misc{richensGeneralAgentsNeed2025,\n title = {General Agents Need World Models},\n author = {Richens, Jonathan and Abel, David and Bellot, Alexis and Everitt, Tom},\n year = {2025},\n doi = {10.48550/arXiv.2506.01622},\n urldate = {2025-06-05},\n number = {arXiv:2506.01622},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/XJACDMMA/Richens et al. - 2025 - General agents need world models.pdf;/home/kellen/Zotero/storage/9PQHSPK2/2506.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.},\n primaryclass = {cs},\n eprint = {2506.01622},\n month = {June}\n}\n"}]