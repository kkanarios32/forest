[{"DOI": "10.48550/arXiv.2311.13294", "URL": "https://arxiv.org/abs/2311.13294", "abstract": "A popular perspective in Reinforcement learning (RL) casts the problem as probabilistic inference on a graphical model of the Markov decision process (MDP). The core object of study is the probability of each state-action pair being visited under the optimal policy. Previous approaches to approximate this quantity can be arbitrarily poor, leading to algorithms that do not implement genuine statistical inference and consequently do not perform well in challenging problems. In this work, we undertake a rigorous Bayesian treatment of the posterior probability of state-action optimality and clarify how it flows through the MDP. We first reveal that this quantity can indeed be used to generate a policy that explores efficiently, as measured by regret. Unfortunately, computing it is intractable, so we derive a new variational Bayesian approximation yielding a tractable convex optimization problem and establish that the resulting policy also explores efficiently. We call our approach VAPOR and show that it has strong connections to Thompson sampling, K-learning, and maximum entropy exploration. We conclude with some experiments demonstrating the performance advantage of a deep RL version of VAPOR.", "accessed": {"date-parts": [[2025, 6, 9]]}, "author": [{"family": "Tarbouriech", "given": "Jean"}, {"family": "Lattimore", "given": "Tor"}, {"family": "O\u2019Donoghue", "given": "Brendan"}], "id": "tarbouriechProbabilisticInferenceReinforcement2023", "issued": {"date-parts": [[2023, 11]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning", "number": "arXiv:2311.13294", "publisher": "arXiv", "title": "Probabilistic Inference in Reinforcement Learning Done Right", "type": "", "original_bibtex": "@misc{tarbouriechProbabilisticInferenceReinforcement2023,\n title = {Probabilistic {{Inference}} in {{Reinforcement Learning Done Right}}},\n author = {Tarbouriech, Jean and Lattimore, Tor and O'Donoghue, Brendan},\n year = {2023},\n doi = {10.48550/arXiv.2311.13294},\n urldate = {2025-06-09},\n number = {arXiv:2311.13294},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/6QTWS9LM/Tarbouriech et al. - 2023 - Probabilistic Inference in Reinforcement Learning Done Right.pdf;/home/kellen/Zotero/storage/YTUL9362/2311.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {A popular perspective in Reinforcement learning (RL) casts the problem as probabilistic inference on a graphical model of the Markov decision process (MDP). The core object of study is the probability of each state-action pair being visited under the optimal policy. Previous approaches to approximate this quantity can be arbitrarily poor, leading to algorithms that do not implement genuine statistical inference and consequently do not perform well in challenging problems. In this work, we undertake a rigorous Bayesian treatment of the posterior probability of state-action optimality and clarify how it flows through the MDP. We first reveal that this quantity can indeed be used to generate a policy that explores efficiently, as measured by regret. Unfortunately, computing it is intractable, so we derive a new variational Bayesian approximation yielding a tractable convex optimization problem and establish that the resulting policy also explores efficiently. We call our approach VAPOR and show that it has strong connections to Thompson sampling, K-learning, and maximum entropy exploration. We conclude with some experiments demonstrating the performance advantage of a deep RL version of VAPOR.},\n primaryclass = {cs},\n eprint = {2311.13294},\n month = {November}\n}\n"}]