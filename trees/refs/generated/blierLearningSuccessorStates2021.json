[{"DOI": "10.48550/arXiv.2101.07123", "URL": "https://arxiv.org/abs/2101.07123", "abstract": "In reinforcement learning, temporal difference-based algorithms can be sample-inefficient: for instance, with sparse rewards, no learning occurs until a reward is observed. This can be remedied by learning richer objects, such as a model of the environment, or successor states. Successor states model the expected future state occupancy from any given state [Dayan, 1993, Kulkarni et al., 2016], and summarize all paths in the environment for a given policy. They are related to goal-dependent value functions, which learn how to reach arbitrary states.", "accessed": {"date-parts": [[2025, 2, 27]]}, "author": [{"family": "Blier", "given": "L\u00e9onard"}, {"family": "Tallec", "given": "Corentin"}, {"family": "Ollivier", "given": "Yann"}], "id": "blierLearningSuccessorStates2021", "issued": {"date-parts": [[2021, 1]]}, "keyword": "Computer Science - Machine Learning", "language": "en-US", "number": "arXiv:2101.07123", "publisher": "arXiv", "title": "Learning Successor States and Goal-Dependent Values: A Mathematical Viewpoint", "title-short": "Learning Successor States and Goal-Dependent Values", "type": "", "original_bibtex": "@misc{blierLearningSuccessorStates2021,\n title = {Learning {{Successor States}} and {{Goal-Dependent Values}}: {{A Mathematical Viewpoint}}},\n author = {Blier, L{\\'e}onard and Tallec, Corentin and Ollivier, Yann},\n year = {2021},\n doi = {10.48550/arXiv.2101.07123},\n urldate = {2025-02-27},\n number = {arXiv:2101.07123},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/LUN44LMX/Blier et al. - 2021 - Learning Successor States and Goal-Dependent Values A Mathematical Viewpoint.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In reinforcement learning, temporal difference-based algorithms can be sample-inefficient: for instance, with sparse rewards, no learning occurs until a reward is observed. This can be remedied by learning richer objects, such as a model of the environment, or successor states. Successor states model the expected future state occupancy from any given state [Dayan, 1993, Kulkarni et al., 2016], and summarize all paths in the environment for a given policy. They are related to goal-dependent value functions, which learn how to reach arbitrary states.},\n primaryclass = {cs},\n eprint = {2101.07123},\n month = {January},\n shorttitle = {Learning {{Successor States}} and {{Goal-Dependent Values}}}\n}\n"}]