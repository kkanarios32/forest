[{"DOI": "10.48550/arXiv.2310.04415", "URL": "https://arxiv.org/abs/2310.04415", "abstract": "Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss and improved training stability. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way. The code is available at https://github.com/tml-epfl/why-weight-decay", "accessed": {"date-parts": [[2025, 2, 26]]}, "author": [{"family": "D\u2019Angelo", "given": "Francesco"}, {"family": "Andriushchenko", "given": "Maksym"}, {"family": "Varre", "given": "Aditya"}, {"family": "Flammarion", "given": "Nicolas"}], "id": "dangeloWhyWeNeed2024", "issued": {"date-parts": [[2024, 11]]}, "keyword": "Computer Science - Machine Learning", "language": "en-US", "number": "arXiv:2310.04415", "publisher": "arXiv", "title": "Why Do We Need Weight Decay in Modern Deep Learning?", "type": "", "original_bibtex": "@misc{dangeloWhyWeNeed2024,\n title = {Why {{Do We Need Weight Decay}} in {{Modern Deep Learning}}?},\n author = {D'Angelo, Francesco and Andriushchenko, Maksym and Varre, Aditya and Flammarion, Nicolas},\n year = {2024},\n doi = {10.48550/arXiv.2310.04415},\n urldate = {2025-02-26},\n number = {arXiv:2310.04415},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/5GJGM755/D'Angelo et al. - 2024 - Why Do We Need Weight Decay in Modern Deep Learning.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss and improved training stability. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way. The code is available at https://github.com/tml-epfl/why-weight-decay},\n primaryclass = {cs},\n eprint = {2310.04415},\n month = {November}\n}\n"}]