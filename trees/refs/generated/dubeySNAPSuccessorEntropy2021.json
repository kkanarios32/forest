[{"DOI": "10.1145/3487983.3488292", "ISBN": "978-1-4503-9131-3", "abstract": "Reinforcement learning (RL) has demonstrated great success in solving navigation tasks but often fails when learning complex environmental structures. One open challenge is to incorporate low-level generalizable skills with human-like adaptive path-planning in an RL framework. Motivated by neural findings in animal navigation, we propose a Successor eNtropy-based Adaptive Path-planning (SNAP) that combines a low-level goal-conditioned policy with the flexibility of a classical high-level planner. SNAP decomposes distant goal-reaching tasks into multiple nearby goal-reaching sub-tasks using a topological graph. To construct this graph, we propose an incremental subgoal discovery method that leverages the highest-entropy states in the learned Successor Representation. The Successor Representation encodes the likelihood of being in a future state given the current state and capture the relational structure of states based on a policy. Our main contributions lie in discovering subgoal states that efficiently abstract the state-space and proposing a low-level goal-conditioned controller for local navigation. Since the basic low-level skill is learned independent of state representation, our model easily generalizes to novel environments without intensive relearning. We provide empirical evidence that the proposed method enables agents to perform long-horizon sparse reward tasks quickly, take detours during barrier tasks, and exploit shortcuts that did not exist during training. Our experiments further show that the proposed method outperforms the existing goal-conditioned RL algorithms in successfully reaching distant-goal tasks and policy learning. To evaluate human-like adaptive path-planning, we also compare our optimal agent with human data and found that, on average, the agent was able to find a shorter path than the human participants.", "accessed": {"date-parts": [[2025, 6, 14]]}, "author": [{"family": "Dubey", "given": "Rohit K."}, {"family": "Sohn", "given": "Samuel S."}, {"family": "Abualdenien", "given": "Jimmy"}, {"family": "Thrash", "given": "Tyler"}, {"family": "Hoelscher", "given": "Christoph"}, {"family": "Borrmann", "given": "Andr\u00e9"}, {"family": "Kapadia", "given": "Mubbasir"}], "collection-title": "MIG \u201921", "container-title": "Proceedings of the 14th ACM SIGGRAPH Conference on Motion, Interaction and Games", "id": "dubeySNAPSuccessorEntropy2021", "issued": {"date-parts": [[2021, 11]]}, "page": "1-11", "publisher": "Association for Computing Machinery", "publisher-place": "New York, NY, USA", "title": "SNAP:Successor Entropy based Incremental Subgoal Discovery for Adaptive Navigation", "title-short": "SNAP", "type": "paper-conference", "original_bibtex": "@inproceedings{dubeySNAPSuccessorEntropy2021,\n title = {{{SNAP}}:{{Successor Entropy}} Based {{Incremental Subgoal Discovery}} for {{Adaptive Navigation}}},\n author = {Dubey, Rohit K. and Sohn, Samuel S. and Abualdenien, Jimmy and Thrash, Tyler and Hoelscher, Christoph and Borrmann, Andr{\\'e} and Kapadia, Mubbasir},\n year = {2021},\n isbn = {978-1-4503-9131-3},\n doi = {10.1145/3487983.3488292},\n urldate = {2025-06-14},\n booktitle = {Proceedings of the 14th {{ACM SIGGRAPH Conference}} on {{Motion}}, {{Interaction}} and {{Games}}},\n series = {{{MIG}} '21},\n pages = {1--11},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n file = {/home/kellen/Zotero/storage/N28BM5HI/Dubey et al. - 2021 - SNAPSuccessor Entropy based Incremental Subgoal Discovery for Adaptive Navigation.pdf},\n abstract = {Reinforcement learning (RL) has demonstrated great success in solving navigation tasks but often fails when learning complex environmental structures. One open challenge is to incorporate low-level generalizable skills with human-like adaptive path-planning in an RL framework. Motivated by neural findings in animal navigation, we propose a Successor eNtropy-based Adaptive Path-planning (SNAP) that combines a low-level goal-conditioned policy with the flexibility of a classical high-level planner. SNAP decomposes distant goal-reaching tasks into multiple nearby goal-reaching sub-tasks using a topological graph. To construct this graph, we propose an incremental subgoal discovery method that leverages the highest-entropy states in the learned Successor Representation. The Successor Representation encodes the likelihood of being in a future state given the current state and capture the relational structure of states based on a policy. Our main contributions lie in discovering subgoal states that efficiently abstract the state-space and proposing a low-level goal-conditioned controller for local navigation. Since the basic low-level skill is learned independent of state representation, our model easily generalizes to novel environments without intensive relearning. We provide empirical evidence that the proposed method enables agents to perform long-horizon sparse reward tasks quickly, take detours during barrier tasks, and exploit shortcuts that did not exist during training. Our experiments further show that the proposed method outperforms the existing goal-conditioned RL algorithms in successfully reaching distant-goal tasks and policy learning. To evaluate human-like adaptive path-planning, we also compare our optimal agent with human data and found that, on average, the agent was able to find a shorter path than the human participants.},\n month = {November},\n shorttitle = {{{SNAP}}}\n}\n"}]