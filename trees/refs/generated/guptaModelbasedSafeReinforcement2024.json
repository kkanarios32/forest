[{"DOI": "10.1145/3632410.3632446", "ISBN": "979-8-4007-1634-8", "abstract": "Safe reinforcement learning aims to ensure the safety of agents and their interactions with the environment. Traditional reinforcement learning algorithms often neglect safety considerations, resulting in undesirable consequences when deployed in real-world scenarios. To address this issue, safe reinforcement learning algorithms incorporate safety constraints or modify the reward distribution to prioritize the safety of the agent. Recent literature on modelbased safe reinforcement learning uses a fixed look-ahead horizon to avoid unsafe states, limiting the agent\u2019s adaptability to changing environments. In this paper, we propose a variable horizon lookahead based on the agent\u2019s current state, resulting in improved performance and adaptability in uncertain environments. Our approach leverages the Gaussian process regression model to estimate the value of the horizon dynamically based on the agent\u2019s current state. To enhance sample efficiency, we introduce a selective sampling strategy that reduces the number of rollouts by eliminating samples that may lead to unsafe states and also optimizes the use of computational resources while ensuring safety. We evaluate the effectiveness of our approach in multiple mujoco environments. Our results demonstrate a significant reduction in terms of safety violations during validating compared to existing approaches from the literature.", "accessed": {"date-parts": [[2025, 3, 17]]}, "author": [{"family": "Gupta", "given": "Shourya"}, {"family": "Suryaman", "given": "Utkarsh"}, {"family": "Narava", "given": "Rahul"}, {"family": "Jha", "given": "Shashi Shekhar"}], "container-title": "Proceedings of the 7th Joint International Conference on Data Science & Management of Data (11th ACM IKDD CODS and 29th COMAD)", "id": "guptaModelbasedSafeReinforcement2024", "issued": {"date-parts": [[2024, 1]]}, "language": "en-US", "page": "100-108", "publisher": "ACM", "publisher-place": "Bangalore India", "title": "Model-based Safe Reinforcement Learning using Variable Horizon Rollouts", "type": "paper-conference", "original_bibtex": "@inproceedings{guptaModelbasedSafeReinforcement2024,\n title = {Model-Based {{Safe Reinforcement Learning}} Using {{Variable Horizon Rollouts}}},\n author = {Gupta, Shourya and Suryaman, Utkarsh and Narava, Rahul and Jha, Shashi Shekhar},\n year = {2024},\n isbn = {979-8-4007-1634-8},\n doi = {10.1145/3632410.3632446},\n urldate = {2025-03-17},\n booktitle = {Proceedings of the 7th {{Joint International Conference}} on {{Data Science}} \\& {{Management}} of {{Data}} (11th {{ACM IKDD CODS}} and 29th {{COMAD}})},\n pages = {100--108},\n publisher = {ACM},\n address = {Bangalore India},\n file = {/home/kellen/Zotero/storage/FBVZY26U/Gupta et al. - 2024 - Model-based Safe Reinforcement Learning using Variable Horizon Rollouts.pdf},\n langid = {english},\n abstract = {Safe reinforcement learning aims to ensure the safety of agents and their interactions with the environment. Traditional reinforcement learning algorithms often neglect safety considerations, resulting in undesirable consequences when deployed in real-world scenarios. To address this issue, safe reinforcement learning algorithms incorporate safety constraints or modify the reward distribution to prioritize the safety of the agent. Recent literature on modelbased safe reinforcement learning uses a fixed look-ahead horizon to avoid unsafe states, limiting the agent's adaptability to changing environments. In this paper, we propose a variable horizon lookahead based on the agent's current state, resulting in improved performance and adaptability in uncertain environments. Our approach leverages the Gaussian process regression model to estimate the value of the horizon dynamically based on the agent's current state. To enhance sample efficiency, we introduce a selective sampling strategy that reduces the number of rollouts by eliminating samples that may lead to unsafe states and also optimizes the use of computational resources while ensuring safety. We evaluate the effectiveness of our approach in multiple mujoco environments. Our results demonstrate a significant reduction in terms of safety violations during validating compared to existing approaches from the literature.},\n month = {January}\n}\n"}]