[{"DOI": "10.48550/arXiv.2101.06840", "URL": "https://arxiv.org/abs/2101.06840", "abstract": "Large-scale model training has been a playing ground for a limited few requiring complex model refactoring and access to prohibitively expensive GPU clusters. ZeRO-Offload changes the large model training landscape by making large model training accessible to nearly everyone. It can train models with over 13 billion parameters on a single GPU, a 10x increase in size compared to popular framework such as PyTorch, and it does so without requiring any model change from the data scientists or sacrificing computational efficiency.", "accessed": {"date-parts": [[2025, 1, 30]]}, "author": [{"family": "Ren", "given": "Jie"}, {"family": "Rajbhandari", "given": "Samyam"}, {"family": "Aminabadi", "given": "Reza Yazdani"}, {"family": "Ruwase", "given": "Olatunji"}, {"family": "Yang", "given": "Shuangyan"}, {"family": "Zhang", "given": "Minjia"}, {"family": "Li", "given": "Dong"}, {"family": "He", "given": "Yuxiong"}], "id": "renZeROOffloadDemocratizingBillionScale2021", "issued": {"date-parts": [[2021, 1]]}, "keyword": "Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning", "language": "en-US", "number": "arXiv:2101.06840", "publisher": "arXiv", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training", "title-short": "ZeRO-Offload", "type": "", "original_bibtex": "@misc{renZeROOffloadDemocratizingBillionScale2021,\n title = {{{ZeRO-Offload}}: {{Democratizing Billion-Scale Model Training}}},\n author = {Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},\n year = {2021},\n doi = {10.48550/arXiv.2101.06840},\n urldate = {2025-01-30},\n number = {arXiv:2101.06840},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/EQNAIMY3/Ren et al. - 2021 - ZeRO-Offload Democratizing Billion-Scale Model Training.pdf},\n keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Large-scale model training has been a playing ground for a limited few requiring complex model refactoring and access to prohibitively expensive GPU clusters. ZeRO-Offload changes the large model training landscape by making large model training accessible to nearly everyone. It can train models with over 13 billion parameters on a single GPU, a 10x increase in size compared to popular framework such as PyTorch, and it does so without requiring any model change from the data scientists or sacrificing computational efficiency.},\n primaryclass = {cs},\n eprint = {2101.06840},\n month = {January},\n shorttitle = {{{ZeRO-Offload}}}\n}\n"}]