[{"DOI": "10.48550/arXiv.2001.00805", "URL": "https://arxiv.org/abs/2001.00805", "abstract": "Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts \u201cRL as inference\u201d and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular \u201cRL as inference\u201d approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.", "accessed": {"date-parts": [[2025, 6, 9]]}, "author": [{"family": "O\u2019Donoghue", "given": "Brendan"}, {"family": "Osband", "given": "Ian"}, {"family": "Ionescu", "given": "Catalin"}], "id": "odonoghueMakingSenseReinforcement2020", "issued": {"date-parts": [[2020, 11]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning", "number": "arXiv:2001.00805", "publisher": "arXiv", "title": "Making Sense of Reinforcement Learning and Probabilistic Inference", "type": "", "original_bibtex": "@misc{odonoghueMakingSenseReinforcement2020,\n title = {Making {{Sense}} of {{Reinforcement Learning}} and {{Probabilistic Inference}}},\n author = {O'Donoghue, Brendan and Osband, Ian and Ionescu, Catalin},\n year = {2020},\n doi = {10.48550/arXiv.2001.00805},\n urldate = {2025-06-09},\n number = {arXiv:2001.00805},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/KKIHIKDE/O'Donoghue et al. - 2020 - Making Sense of Reinforcement Learning and Probabilistic Inference.pdf;/home/kellen/Zotero/storage/YK9BPL3L/2001.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts `RL as inference' and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular `RL as inference' approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.},\n primaryclass = {cs},\n eprint = {2001.00805},\n month = {November}\n}\n"}]