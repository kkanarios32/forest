[{"DOI": "10.48550/arXiv.2501.02709", "URL": "https://arxiv.org/abs/2501.02709", "abstract": "We study goal-conditioned RL through the lens of generalization, but not in the traditional sense of random augmentations and domain randomization. Rather, we aim to learn goal-directed policies that generalize with respect to the horizon: after training to reach nearby goals (which are easy to learn), these policies should succeed in reaching distant goals (which are quite challenging to learn). In the same way that invariance is closely linked with generalization is other areas of machine learning (e.g., normalization layers make a network invariant to scale, and therefore generalize to inputs of varying scales), we show that this notion of horizon generalization is closely linked with invariance to planning: a policy navigating towards a goal will select the same actions as if it were navigating to a waypoint en route to that goal. Thus, such a policy trained to reach nearby goals should succeed at reaching arbitrarily-distant goals. Our theoretical analysis proves that both horizon generalization and planning invariance are possible, under some assumptions. We present new experimental results and recall findings from prior work in support of our theoretical results. Taken together, our results open the door to studying how techniques for invariance and generalization developed in other areas of machine learning might be adapted to achieve this alluring property.", "accessed": {"date-parts": [[2025, 6, 14]]}, "author": [{"family": "Myers", "given": "Vivek"}, {"family": "Ji", "given": "Catherine"}, {"family": "Eysenbach", "given": "Benjamin"}], "id": "myersHorizonGeneralizationReinforcement2025", "issued": {"date-parts": [[2025, 1]]}, "keyword": "Computer Science - Artificial Intelligence,Computer Science - Machine Learning", "number": "arXiv:2501.02709", "publisher": "arXiv", "title": "Horizon Generalization in Reinforcement Learning", "type": "", "original_bibtex": "@misc{myersHorizonGeneralizationReinforcement2025,\n title = {Horizon {{Generalization}} in {{Reinforcement Learning}}},\n author = {Myers, Vivek and Ji, Catherine and Eysenbach, Benjamin},\n year = {2025},\n doi = {10.48550/arXiv.2501.02709},\n urldate = {2025-06-14},\n number = {arXiv:2501.02709},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/TAKJZ8JS/Myers et al. - 2025 - Horizon Generalization in Reinforcement Learning.pdf;/home/kellen/Zotero/storage/4FIJY9TU/2501.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {We study goal-conditioned RL through the lens of generalization, but not in the traditional sense of random augmentations and domain randomization. Rather, we aim to learn goal-directed policies that generalize with respect to the horizon: after training to reach nearby goals (which are easy to learn), these policies should succeed in reaching distant goals (which are quite challenging to learn). In the same way that invariance is closely linked with generalization is other areas of machine learning (e.g., normalization layers make a network invariant to scale, and therefore generalize to inputs of varying scales), we show that this notion of horizon generalization is closely linked with invariance to planning: a policy navigating towards a goal will select the same actions as if it were navigating to a waypoint en route to that goal. Thus, such a policy trained to reach nearby goals should succeed at reaching arbitrarily-distant goals. Our theoretical analysis proves that both horizon generalization and planning invariance are possible, under some assumptions. We present new experimental results and recall findings from prior work in support of our theoretical results. Taken together, our results open the door to studying how techniques for invariance and generalization developed in other areas of machine learning might be adapted to achieve this alluring property.},\n primaryclass = {cs},\n eprint = {2501.02709},\n month = {January}\n}\n"}]