% ["refs"]
\title{LIPSCHITZ-CONSTRAINED UNSUPERVISED SKILL DISCOVERY}
\date{2022}
\author/literal{Seohong Park}\author/literal{Jongwook Choi}\author/literal{Jaekyeom Kim}\author/literal{Honglak Lee}\author/literal{Gunhee Kim}
\taxon{Reference}
\meta{bibtex}{\startverb
@article{parkLIPSCHITZCONSTRAINEDUNSUPERVISEDSKILL2022,
 title = {{{LIPSCHITZ-CONSTRAINED UNSUPERVISED SKILL DISCOVERY}}},
 author = {Park, Seohong and Choi, Jongwook and Kim, Jaekyeom and Lee, Honglak and Kim, Gunhee},
 year = {2022},
 file = {/home/kellen/Downloads/pdfs/storage/EZ9YLY62/Park et al. - 2022 - LIPSCHITZ-CONSTRAINED UNSUPERVISED SKILL DISCOVERY.pdf},
 langid = {english},
 abstract = {We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another benefit of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner --- i.e., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at https://shpark.me/projects/lsd/.}
}
\stopverb}