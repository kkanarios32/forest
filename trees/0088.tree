\import{base-macros}
\date{2025-07-01T17:03:45Z}
\title{Expectation Maximization}
\author{kellenkanarios}

\p{This is the follow up to the previous note on the [ELBO](0082). Mainly, here we aim to show how we can use the ELBO to find #{\theta^*} i.e. 
##{
\begin{align*}
 \theta^* &= \arg\max \mathbb{E}_{x \sim \mathcal{D}} \log p(x; \theta) \\ 
 &= \arg\max \mathbb{E}_{x \sim \mathcal{D}} \log \int p(x, z; \theta) \mathrm{d}z 
.\end{align*}
}
Notably, [ELBO](0082) gives us the lower bound 
##{\log p(x; \theta) \geq \mathbb{E}_{z \sim q}\left[\log \frac{p(x,  Z; \theta)}{q(z)}\right]  \eqqcolon \mathsf{ELBO}(x, q, \theta)}
for a \strong{fixed} theta \strong{and} #{q}. Therefore, \strong{for each} #{\theta }, we want to find some #{q}, such that
  ##{\mathsf{ELBO}(x, q, \theta) = \log p(x, \theta)}
  With this, we can easily perform gradient ascent
  ##{\theta_{t + 1} \gets \theta_t + \nabla_{\theta} \mathsf{ELBO}(x, q, \theta_t),}
  to maximize the log-likelihood #{\log p(x; \theta)}. 
}
\remark{
Note that the gradient is with respect to #{\theta}, where #{\theta} is just the third argument to #{\mathsf{ELBO}} as defined above. Notably, #{\theta_t} will show up elsewhere later but will \strong{NOT} be backpropagated through.
}
\section{Expectation}{
\p{The above motivates the \em{expectation} step of the EM algorithm. Namely how can we compute the lower bound expectation for the \strong{best} possible #{q}.}
\p{Recall (or review) from the discussion on [ELBO](0082) that #{q(z) = p(z | x; \theta)} maximizes the ELBO for a given #{x} and #{\theta }. Therefore, we actually want to learn #{q_{\phi}(z, x, \theta) \approx p(z | x; \theta)}. To do so, we can simply maximize the ELBO (duh). Formally,
##{\phi_{t+1} = \argmax_{\phi} \mathbb{E}_{x \sim \mathcal{D}, z \sim q}\left[\log \frac{p(x,  Z; \theta_t)}{q_{\phi}(Z, x, \theta_t)}\right]}
}
\question{
Now this is all cool theoretically but how TF do we actually maximize this thing? 
}
\answer{
  It is highly dependent on the parametrization of #{p} and #{q}.
}
\example{
  \p{
  We define the problem as 
  \ol{
    \li{#{p(z) \sim N(0, 1)}}
    \li{#{p(x | z; \theta) \sim N(\mu_{\theta}(z), \sigma_{\theta}(z))}}
    \li{#{q_{\phi}(z | x, \theta) \sim N(\mu_{\phi}(x), \sigma_\phi(x))}}
  }
  where #{\mu_{\{\phi, \theta\}}, \sigma_{\{\phi, \theta\}}} are parametrized neural networks. Then we can rewrite the [ELBO](0082) as (ignoring constants)
  ##{
    \begin{align*}
     \mathsf{ELBO}(x, q, \theta) &= \mathbb{E}_{x \sim \mathcal{D}, z \sim q(\cdot \mid x, \theta)} \left[\log p(Z) - \log \frac{p(x \mid Z; \theta)}{q_{\phi}(Z, x, \theta)}\right] \\
&= \mathbb{E}_{x \sim \mathcal{D}, z \sim q(\cdot \mid x, \theta)} \left[-\frac{Z^2}{2}  - \frac{\sigma_{\phi}(x)^2}{\sigma_{\theta}(z)^2}\big((Z - \mu_{\theta}(Z))^2 - (Z - \mu_{\phi}(x))^2\big)\right]
    .\end{align*}
    }
    We face one more obstacle.
  }
  \question{
    We need to sample realization #{z \sim q_{\phi}(\cdot |x, \theta)}. So isn't our realization #{z} a function of #{\phi}. How would we optimize this?
  }
  \answer{
    The [reparametrization trick](0089)!!
  }
  \transclude{0089}
  \p{Now for a given sample #{x_i \sim \mathcal{D}}, we can get a sample #{z_i(\phi) \sim q(\cdot \mid x_i, \theta)} that is differentiable with respect to #{\phi}. Using this, we can get the stochastic gradient estimate
  ##{\nabla_q \widetilde{\mathsf{ELBO}}(x, q, \theta) = \nabla_{\phi} \left[\frac{z_i(\phi)^2}{2}  - \frac{\sigma_{\phi}(x)^2}{\sigma_{\theta}(z)^2}\big((z_i(\phi) - \mu_{\theta}(z_i(\phi)))^2 - (z_i(\phi) - \mu_{\phi}(x))^2\big)\right]}
  }
\iblock{
  \strong{Algorithm:} input #{\theta_t}
  \ol{
    \li{Sample #{x_i} from dataset #{\mathcal{D}}}
    \li{Sample #{z_i(\phi) \sim q_{\phi}(\cdot \mid x_i, \theta_t)} via [trick](0089).}
    \li{Compute #{\nabla_q \widetilde{\mathsf{ELBO}}}}
    \li{Update via SGD}
  }
}
}
}
\section{Maximization}{
\p{Finally, we have reached the maximization step. This step is fairly straightforward after all the progress we have made. Once we have a tight lower bound i.e. #{q(z) \approx p(z \mid x; \theta)} then we simply fix #{q} and maximize the [ELBO](0082) for #{\theta} i.e. in the above example and algorithm we instead compute
##{\nabla_\theta \widetilde{\mathsf{ELBO}}(x, q, \theta) = \nabla_{\theta} \left[\frac{z_i(\phi)^2}{2}  - \frac{\sigma_{\phi}(x)^2}{\sigma_{\theta}(z)^2}\big((z_i(\phi) - \mu_{\theta}(z_i(\phi)))^2 - (z_i(\phi) - \mu_{\phi}(x))^2\big)\right]}
Alternating between these steps yields the \em{expectation-maximization} algorithm the ML oldheads mythologize.
}
}
