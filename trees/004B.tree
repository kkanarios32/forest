\date{2025-02-01}
\import{base-macros}
\title{Notebook: Information Theory}
\author{kellenkanarios}
\tag{note}
\tag{top}
\xmlns:html{http://www.w3.org/1999/xhtml}
\section{Definitions}{
\transclude{0081}
}

\section{Lecture 5}{
\transclude{004C}
\transclude{004D}

\transclude{0034}
\remark{
    This can be thought of as the tangent is always an underestimate of a convex function.
  }
\transclude{0035}
\remark{
  This implies positive curvature or the common "bowl-shaped" interpretation.
  }
\transclude{002G}
\remark{
    Holds with equality if and only if 
    \ol{
      \li{#{f} is linear on #{\mathcal{X}}}
      \li{#{X} is a constant almost surely.}
      }
  }
\transclude{004E}
\problem{
  When is #{D_{\mathrm{KL}}(P || Q) = 0}?
}
\answer{
    Need #{Q(x) / P(x)} to be constant almost surely. This occurs only when #{Q = P}. 
    ##{
        Q(x) / P(x) = c \implies Q(x) = P(x) c \iff \sum Q(x) = c \sum P(x) \implies c = 1
      }
  }
  \strong{Applications of [[004E]]:}
  \ul{
    \li{
      #{I(X; Y) \geq 0}
    }
    \li{#{H(X | Y) \leq H(X)}}
    \li{#{H(X^n) \leq \sum_{i = 1}^n H(X_i)}}
    \li{#{H(X) \leq \log |\mathcal{X}|}}
  }
  \transclude{004F}
  \strong{Consequences of [[004F]]:}
  \ul{
      \li{Joint convexity of #{D_{\mathrm{KL}}(P || Q)},}
      \li{Individual convexity of #{D_{\mathrm{KL}}(P || Q)}.}
      \li{Concavity of entropy.}
    }
}
\section{Lecture 7}{
\transclude{004Y}
\section{Data Processing Inequality}{
    High-level
    \ul{
        \li{#{Y} contains more information about #{X} than #{\hat{X}}.}
        \li{We cannot increase the amount of information about #{X} by processing #{Y}.}
      }
    \strong{Markov Chains:}
    \ul{
        \li{#{(U, V, W)} form a Markov chain if
        ##{
\begin{align*}
  P(W = w \mid U = u, V = v) = P(w = w \mid V = v)
\end{align*}
          }
Denoted #{U \to V \to W}.
        }
          \li{Equivalently, we say #{(U \perp W) | V} which means ##{
             P(W = w, U=u \mid V = v) \implies P(W = w \mid V = v)P(U = u \mid V = v)
          }
          }
          \li{Reversibility: #{U \to V \to W \iff W \to V \to U}.}
          \li{Estimation: #{X \to Y \to \hat{X}}}
      } 

\transclude{004Z}
}
\section{Fano's Inequality}{
    \ul{
        \li{#{H(X | Y)} controls the error probability #{P_e}.}
      }
\transclude{0050}
}
  }
\section{Asymptotic Equipartion Propery}{
\transclude{0051}
\transclude{0052}
\transclude{0053}
\transclude{0054}
\transclude{0055}
\p{
\strong{interpretation:} #{A_{\epsilon}^{(n)}\,\text{ is a subset of }\,{\mathcal{X}}^{n}}
\ul{
    \li{Contains almost all the probability.}
    \li{Consists of #{\approx 2^{nH(X)}}}
    \li{Almost equiprobable sequences.}
  }
}
\transclude{0056}
}

\section{Fixed-rate lossless source coding}{
\transclude{0059}
\p{
    \strong{Performance:}
    \ul{
        \li{Rate = #{\frac{\log_2 \theta}{n}}}
        \li{Probability of decoding error = #{\mathbb{P}|\hat{X}^n \neq X^n|}}
      }
  }
\transclude{005A}
\transclude{005B}
\transclude{005C}
  }

\section{Variable Rate Source Coding}{
\transclude{005K}
\transclude{005L}
\remark{Although it is unclear here, the difference between variable and fixed rate source coding is that we allow our encoder and decoder to map to/from #{\{0,1\}^* }, where #{^*} is variable i.e. our encoded versions of our alphabet can have varying lengths.}
\transclude{005U}
\transclude{005M}
\transclude{005N}
\remark{[Prefix free](005N) codes are [uniquely decodable](005M). To show unique-decodability, you simply have to provide a way to decode and show it is unique. For [prefix-free](005N), greedily decoding suffices.}
\figure{
\<html:img>[width]{300px}[src]{img/types-of-codes.png}{}
\figcaption{Taxonomy of codes from \ref{coverELEMENTSINFORMATIONTHEORY}.}
}
\remark{
Note that [non-singular](005U) does not imply [uniquely decodable](005M). This is because for [uniquely decodable](005M) we require the extension to be [non-singular](005U). For example, if we have that #{e(0) = 0, e(1) = 1, e(2) = 01}. Then #{e^*(01) = e^*(2)}. From this, we can see that [uniquely decodable](005M) is a much stronger condition.
}
}


\section{Kraft's inequality}{
\transclude{005O}
\remark{Other direction is also true. For any #{\ell_i} satisfying Kraft's inequality, one can construct a prefix-free code whose lengths of #{e(x_i) = \ell_i}.}
\remark{Surprisingly, any [uniquely decodable](005M) code must also satisfy [[005O]].}
    }
\section{Performance Limit of Variable Rate Source Codes}{
\transclude{005P}
\transclude{005Q}
\transclude{005R}
\transclude{005S}
}
\transclude{005T}
\transclude{005Z}

\section{Method of Types}{
    \transclude{005X}
    \transclude{005Y}
    \transclude{0061}
\example{
    Let #{\mathcal{X} = \{1,2,3\}.} For #{n = 2}, #{x^n = 12} the [type](005X) is #{(\frac{1}{2},\frac{1}{2}, 0)} and the [type class](0061) is #{\{12, 21\}  }.
}
\section{Properties}{
\transclude{0062}
\transclude{0063}
\transclude{0065}
\transclude{0064}
\transclude{0066}
}
}
\section{Large Deviation Theory}{
\transclude{0067}
}

\section{Hypothesis testing}{
\transclude{0069}
\transclude{006A}
\transclude{006B}
\section{Notions of optimality}{
\transclude{006C}
\transclude{006D}
\transclude{006E}
}
\section{Optimal NP Test}{
\transclude{0068}
\transclude{006F}
}
\section{Error Exponents in Hypothesis Testing}{
In the previous section, we consider the optimal test in the non-asymptotic regime i.e. for finite #{n} what test minimizes the error according to some notion of error. In this section, we ask the question of what is the rate that this error converges to #{0}. More specifically, if #{\text{error}_n \leq 2^{-nV}} then what is the best (largest) #{V} we can hope to achieve?
\section{NP Error Exponent}{
\transclude{006G}
\transclude{006H}
}
\section{Bayesian Error Exponent}{
\transclude{006I}
\transclude{006J}
}
}
}
\section{Fixed-Rate Universal Source Coding}{
\transclude{006K}
\section{Construction of universal fixed rated code}{
    To construct, a universal fixed rate code, we need a code that satisfies (i) and (ii) from [[006K]]. 
    \ul{
        \li{
Let #{ |\mathcal{X}|=m }, and for any #{ n\geq1 } and a given #{ R>0 }, define #{ R_n=R-m \frac{\log n+1}{n}}
            }
      \li{Consider the subset #{A_n \subset \mathcal{X}^n}, defined as ##{A_n = \{x^n \in \mathcal{X}^n \mid H(\hat{P}_{x^n} \leq R_n)\}  }}
      \li{Then just define the encoder in the usual way i.e. for encoder #{e_n} give every #{x^n \in A_n} a unique code and everything else map to the same code.}
      \li{Then the decoder can easily identify everything in #{A_n} and it suffices to show that in the limit the probabilities concentrate around #{A_n}.}
    }
\transclude{006L}
}
}
\section{Variable-Rate Universal Source Coding}{
\transclude{006M}
\section{Redundancy-Capacity Theorem}{
\transclude{006N}
\remark{
It will turn out the optimal choice of #{Q} is the centroid associated with the #{k} elements of #{\mathcal{P}}.
}
\transclude{006O}
}
\transclude{006P}
}
\section{Channel Coding}{
\transclude{006S}
\transclude{006T}
\transclude{006U}
\transclude{006V}
\remark{
    We make the following simplifying assumptions (for now)
    \ul{
        \li{
We assume no feedback. This means that if we have #{W \to X_{1} \to Y_{1} \to X_{2} \to \cdots} then #{X_i} does not use the knowledge of previous channel outputs i.e. #{Y^{i - 1} \leftrightarrow (W, X^{i - 1}) \leftrightarrow X_i}.
            }
            \li{
Random encoding: #{X^n(i)} is drawn i.i.d from some specified distribution #{P_X}.
                }
    }
}
\transclude{006W}
\transclude{006X}
\transclude{006Y}
\remark{
The capacity of a [symmetric or weakly symmetric](006Y) channel is #{\log(|\mathcal{Y}|) - H(r)}, where #{r} is the first row of #{P_{Y|X}}.
}
}
\section{Channel Coding}{
    \p{We have three components to the channel coding problem:}
    \ol{
      \li{Channel encoder}
      \li{Noisy channel}
      \li{Channel decoder}
    }
    \p{We first pass #{W} to the channel encoder get some #{X^n} which goes into noisy channel and becomes #{Y^n}. Then the goal is for the channel decoder to recover #{W} from #{Y^n}. This means that the goal of the encoder is to reduce redundancy while also ensuring that there is no overlap when passed through the noisy channel.}
\transclude{0070}
\transclude{0071}
\remark{
We can decompose the joint distribution #{P_{X^n, Y^n}} in terms of the what depends on the channel and the what depends on the encoder. Namely,
##{P_{X^n, Y^n} = \prod_{i=1}^{n} P_{X_i | X^{i - 1}, Y^{i - 1}} \prod_{i=1}^{n}  P_{Y_i | X^{i}, Y^{i - 1}}}
Note that the first term depends only on the encoder and the second term depends only on the channel.
}
\p{
\ul{
\strong{Assumptions:}
  \li{The encoder is a deterministic function of #{\mathbf{W}}}
  \li{The channel does not have access to the message #{\mathbf{W}}.}
}
}
}
\section{Differential Entropy}{
\transclude{007C}
\section{AEP for Continuous Random Variables}{
\transclude{007E}
\transclude{007D}
\transclude{007F}
}
\section{Discrete Approximation}{
\transclude{007G}
\remark{
Taking #{\Delta = 2^{-n}}, then #{X^{\Delta}} is an "#{n}-bit" approximation of #{X}.
}
\transclude{007H}
}
}
\section{Gaussian Channel}{

}
