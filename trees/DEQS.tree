\import{base-macros}
\date{2025-07-03T23:06:23Z}
\title{[[007U]] (4/66): [CS336](0085) Lecture 2}
\author{kellenkanarios}
\section{Motivating Questions}{
  \p{We start by discussing a few essential questions one hopes to answer when they set out to train a large model.}
\question{How long to train a 70b parameter model on 15T tokens on 1024 H100s?
}
\answer{Mysteriously, we spawn from the ether (will be explained later) that \code{total_flops = 6 * 70e9 * 15e12}. Intuitively, we know that we need to perform some computation for each parameter for each token. However, the 6 is still an ominous mystery. Given the total flops, we then can just take the GPUs FLOP/s multiply by the number of seconds in the day and divide \code{total_flops} by this number i.e. 
\code{days = total_flops / flops_per_day}
}
\remark{
In practice, we must also consider the \em{model flop utilization} (MFU). Basically, our algorithm cannot utilize the entirety of the maximum FLOP/s provided by the hardware due to things like memory bandwidth, etc. To account for this in the calculation, they just take \code{flops_per_day / 2}.
}

\question{
  What is the largest model that you can train on 8 H100s using AdamW?
}
\answer{
  An H100 has 80gb of shared memory i.e. \code{h100_bytes = 80e9}. The calculation then proceeds as
  \pre\verb<<<|
# parameters, gradients, optimizer state
bytest_per_param = 4 + 4 + (4 + 4)
num_parameters = (h100_bytes * 8) / bytes_per_parameter
  <<<
  what we need each of parameters, gradients, optimizer state will become clear later.
}
\remark{
In practice, we also need to account for activations, which will depend on \strong{both} batch size and sequence length.
}
  }
\transclude{DEQU}
