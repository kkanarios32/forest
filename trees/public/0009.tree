\tag{public}

\date{2024-10-31}
\title{Contrastive Learning}
\import{latex-preamble}
\import{base-macros}
\p{Recall the longstanding objective of machine learning i.e. given data #{\mathcal{X}} find the distribution #{p(x)} that generated the data #{\mathcal{X}}. The most fundamental approach is \em{maximum likelihood estimation}. Namely, we want to maximize the \em{log-likelhood} of the data
##{\max_{\theta} \sum_{x \in \mathcal{X}} \log p_{\theta}(x) }
}
\section{NCE}{
\p{Suppose we are given #{N} dog samples #{\mathcal{D}} from some distribution #{p_d(x)}. We want to learn #{\theta} so that #{p_{\theta}(x) \approx p_d(x)}. The idea is to learn to distinguish the dogs from our dataset and random samples according to some noisy distribution #{g(x)}. We can now return to the supervised learning setting, where we treat #{\mathcal{D}} and #{\mathcal{R}} as two classes. If we recall standard supervised learning practice, given a sample #{x}, we can easily estimate ##{p(x \text{ is a dog}) = 1 - p(x \text{ is random noise})} 
via binary classification.
}
\p{
Explicitly, we will use logistic regression with the parametrization ##{p_{\theta}(x) = \frac{1}{1 + e^{-G_{\theta}(x)}}.} However, solving the logistic regression problem will yield #{p_{\theta^*}(x) \approx p(x \text{ is a dog})}. Recall that we want to estimate #{p_d(x)} which is the distribution over the "dog manifold".
}
\p{ To estimate the correct quantity, we need to leverage our knowledge of the noise distribution. Recall that if #{p_{\theta}(x) = p(\mathcal{D} \mid x)} then #{G_{\theta}(x) = \log \frac{p(x \mid \mathcal{D})}{p(x \mid \mathcal{R})}}. Since we generated the samples from #{\mathcal{R}}, we have the explicit distribution i.e. #{p(x \mid \mathcal{R}) = p_{\mathcal{R}}(x)}. Therefore, we can restrict #{G_{\theta}} to explicitly learn #{p(x \mid \mathcal{D})} by considering ##{G_{\theta}(x) = \log p_{\theta}(x \mid \mathcal{D}) - \log p_{\mathcal{R}}(x),} considering the cross entropy loss we get the [NCE loss](000E)
}
\transclude{000E}

\p{In the [NCE paper](gutmannNoisecontrastiveEstimationNew2010), they show under mild conditions that the estimator #{p_{\theta}(x \mid D) \to p_{\mathcal{D}}(x)} in probability as the number of samples in the loss goes to infinity. Equivalently, the estimator is [consistent](000F).}
}

\section{Temporally Contrastive Learning in Time Series}{
    \p{
Before we get to contrastive RL, it is a natural question to wonder how does this apply to temporal sequences? Concretely, we want to make predictions about the future given the current "context". However, we want to do so in an unsupervised way, meaning we are only given trajectories not a notion of what it means for a trajectory to be good. Naively, one can try to do this in a supervised manner.
    }
\p{
For a #{k} step prediction, this would just be your model predicting what will happen in #{k} steps then seeing if it matches what occured #{k} steps in the future in the sample trajectory. However, if your sample space #{\mathcal{X}} is very high-dimensional, modeling this relationship can require an exorbinant amount of trajectories.}
\p{This is a central question in the [InfoNCE paper](oordRepresentationLearningContrastive2019), where they propose the following loss:}
\transclude{000B}
}

\p{Now we need to unpack this very ominous loss. To start, what are #{x_k} and #{c_t}?}

\transclude{000C}
\transclude{P658}
