\import{base-macros}
\date{2025-08-17T18:49:16Z}
\tag{public}
\title{Contrastive Learning as GCRL}
##{
\operatorname*{max}_{f(u,v)}\mathbb{E}_{(u,v^{+})\sim p(u,v)}\left[\log\sigma(\underbrace{f(u,{\green v^{+}})}_{\phi(u)^{T}\psi({\green v^{+}})})+\log(1-\sigma(\underbrace{f(u,{\red v^{-}})}_{\phi(u)^{T}\psi({\red v^{-}})}))\right]
}

##{
\begin{align*}
&\operatorname*{max}_{f}\mathbb{E}_{(s,a)\sim p(s,a),s_{f}^{-}\sim p(s_{f})}\left[\mathcal{L}(s,a,s_{f}^{+},s_{f}^{-})\right] \\
\end{align*}
}

##{
\mathcal{L}_1(\theta) = \log\sigma(f_{\theta}(s_1,a_1,{\color{green} s_{8}})) + \log(1-\sigma(f_{\theta}(s_1,a_1, {\color{red} s_3})))
}

##{
\begin{align*}
\widehat{\mathcal{L}}(\theta) &= \frac{1}{n} \sum_{i = 1}^{n} \mathcal{L}_i \\
&= \frac{1}{n} \sum_{i = 1}^{n} \Big[\log\sigma(f_{\theta}(s_i,a_i,{\color{green} s_{f}^{+}})) + \log(1-\sigma(f_{\theta}(s_i,a_i, {\color{red} s_{f}^{-}})))\Big]
\end{align*}
}

##{
\mathcal{L}(\theta) = \mathbb{E}_{x \sim p_X, y \sim p_Y}\Big[\log\sigma(f_{\theta}(x)) + \log(1-\sigma(f_{\theta}(y)))\Big]
}

##{f^*(s, a, s_g) = \log\left(\frac{p^{\pi(\cdot \mid \cdot)}(s_g \mid s, a)}{p(s_g)}\right)}

\proof{
    We want to maximize
##{
    \begin{align*}
\mathcal{L}(\theta) &= \mathbb{E}_{x \sim p_X, y \sim p_Y}\Big[\log\sigma(f_{\theta}(x)) + \log(1-\sigma(f_{\theta}(y)))\Big] \\
&= \int \log\sigma(f_{\theta}(x)) P_X(x) + \int \log(1-\sigma(f_{\theta}(y))) P_Y(y) \\
&= \int \log\sigma(f_{\theta}(z)) P_X(z) + \log(1-\sigma(f_{\theta}(z))) P_Y(z)
    \end{align*}
    }
    Since we are maximizing #{f(s)}, we can just maximize the integrand i.e.
##{
      \begin{align*}
        \frac{\mathrm{d}}{\mathrm{d}f(z)} \Big[\log\sigma(f_{\theta}(z)) P_X(z) + \log(1-\sigma(f_{\theta}(z))) P_Y(z)\Big] = 0
      \end{align*}
    }
    Solving,
    ##{
        \begin{align*}
          P_X(z)\big(1 - \sigma(f(z))\big) - P_Y(z)\sigma(f(z)) = 0 &\iff \sigma(f(z)) = \frac{P_X(z)}{P_X(z) + P_Y(z)} \\
          &\iff f(z) = \log\left(\frac{P_X(z)}{P_Y(z)}\right)
        \end{align*}
      }
  }

  \proof{
The first step is to prove that the average Q-values are close to the task-conditioned Q-values. Below, we will use #{R_{c}(\tau)\triangleq\sum_{\ell=0}^{\infty}\gamma^{\ell}r_{\ell}(s_{\ell},a_{\ell})}:

##{
\begin{align*}
\left|Q^{\beta(\cdot|\cdot,a)}(s,a,e)-Q^{\beta(\cdot|\cdot,\epsilon^{\prime})}(s,a,e)\right|&=\left|\int\beta(\tau\mid s,a,e)R_{e}(\tau)d\tau-\int\beta(\tau\mid s,a,e^{\prime})R_{e}(\tau)d\tau\right|\\ 
&=\left|\int\beta(\tau\mid s,a,e)-\beta(\tau\mid s,a,e^{\prime})R_{e}(\tau)d\tau\right| \\
&=\left|\int\beta(\tau\mid s,a,e)\left(1-\frac{\beta(\tau\mid s,a,e^{\prime})}{\beta(\tau\mid s,a,e)}\right)R_{e}(\tau)d\tau\right| \\
&\leq\int\left|\beta(\tau\mid s,a,e)\left(1-\frac{\beta(\tau\mid s,a,e^{\prime})}{\beta(\tau\mid s,a,e)}\right)\right|d\tau\cdot\operatorname*{max}_{\tau}|R_{e}(\tau)d\tau| \\
&\leq\int\beta(\tau\mid s,a,e)\left|1-\frac{\beta(\tau\mid s,a,e^{\prime})}{\beta(\tau\mid s,a,e)}\right|d\tau\cdot 1 \\
&=\mathbb{E}_{\beta(\tau|s,a,e)}\left[\left|1-{\frac{\beta(\tau\mid s,a,e^{\prime})}{\beta(\tau\mid s,a,e)}}\right|\right] \\
&\leq \epsilon.
\end{align*}
  }
}
