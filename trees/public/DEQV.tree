\tag{public}
\import{base-macros}
\date{2025-07-03T23:41:25Z}
\title{Low Precision Data Types}
\p{\code{float16}: here, we decrease both the fraction and exponent bits. Importantly, there are only #{5} exponent bits making the effective range #{(2^{-6}, 2^{6})}. The dynamic range for small numbers is pretty bad. This can cause small numbers to "die" effectively killing these neurons for the remainder of learning due to backpropagation.
}
\p{\code{bfloat16}: to fix the problems with \code{float16}, they instead keep #{8} exponent bits and reduce the fraction bits to #{7}. This maintains the same dynamic range as \code{float32} while still only using #{16} bits. 
}
\p{\code{fp8}: Two variants
\ol{
\li{E4M3: has 4 exponent bits and 3 mantissa bits} 
\li{E5M2: has 5 exponent bits and 2 mantissa bits} 
}
Requires modern Hopper Architecture i.e. H100 to perform operations. Also supported by TPUs.
}
