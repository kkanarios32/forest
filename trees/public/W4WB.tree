\date{2025-07-13T14:26:13Z}
\tag{public}
\import{base-macros}
\author{kellenkanarios}
\title{Architecture Variations}
\section{Norms}{
\p{\strong{Pre-norm vs. Post-norm}: The first architecture variation discussed is when to apply the [layer norm](LVV4). As can be seen in the figure below, rather than apply the [layer norm](LVV4) in the residual stream. They instead place it before the FFN and MHA layers.}
\figure{
\<html:img>[width]{40\%}[src]{\route-asset{assets/img/pre-post-norm.png}}{}
\figcaption{Post norm (a) vs. pre norm (b).}
}
\p{They actually found that adding [layer normalization](LVV4) before and after the MHA and FFN works the best. This is known as \em{double norm}.}
\p{\strong{[RMSNorm](1XNO) vs. [LayerNorm](LVV4)}:
Another useful trick is to use the [RMSNorm](1XNO) instead of [LayerNorm](LVV4).
}
\transclude{1XNO}
\p{One might wonder if matrix multiplies are the only thing that matters what can such a small change really accomplish.}
\ul{
  \li{Due to memory movement, despite being .17\startverb%\stopverb of [FLOPS](4K5H), provided 25\startverb%\stopverb speedup in runtime!!}
}
\remark{
Most people apparently just drop the bias term and keep the centering i.e. subtracting the mean. This makes sense because computing the mean does not require loading any additional information back to memory.
}
}
\section{Activations}{
  \p{Despite a long list of activations, the two focused on are [ReLU](BEA2) and [GELU](5SPM).}
  \transclude{BEA2}
  \transclude{5SPM}
  \transclude{OZD8}
  \remark{
  The extra computation required by [GELU](5SPM) or [Swish](OZD8) is a non-factor because [FLOPS](4K5H) are dominated by matrix multiplication and there is no increase in memory pressure.
  }
  \transclude{K2N7}
}
\section{Serial vs. Parallel}{
Another small trick is to parallelize computation. Traditionally, one computes the #{\mathrm{MHA}} output and then passes this to the #{\mathrm{FFN}} i.e.
##{\mathbf{y} = \mathbf{x} + \mathrm{FFN}(\mathrm{LN}(\mathbf{x} + \mathrm{MHA}(\mathrm{LN}(\mathbf{x})))).}
However, this requires waiting for the #{\mathrm{MHA}} to complete before performing the #{\mathrm{FFN}} computation. It has been found that performing these in parallel does not cause any severe degradation in performance. Explicitly, instead they do
##{\mathbf{y} = \mathbf{x} + \mathrm{FFN}(\mathrm{LN}(\mathbf{x})) + \mathrm{MHA}(\mathrm{LN}(\mathbf{x}))}
}
